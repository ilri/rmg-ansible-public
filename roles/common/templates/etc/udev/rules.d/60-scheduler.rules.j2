# set noop scheduler for non-rotating disks (SSD)
# the glob matches sd* on physical hosts, xvd* on Xen hosts, and vd* on KVM hosts
ACTION=="add|change", KERNEL=="xvd[a-z]|[sv]d[a-z]", ATTR{queue/rotational}=="0", ATTR{queue/scheduler}="noop"

# set deadline scheduler for rotating disks
# the glob matches sd* on physical hosts, xvd* on Xen hosts, and vd* on KVM hosts
# 
# newer KVM guests (Ubuntu 14.04 at least, so Linux >=3.13) using virtio vda don't
# allow it to be set, but it's non fatal so let's just set it for now
#
# NOTE: Linode KVM guests use sd* and report '1' for rotational. We need to figure out how this affects us
ACTION=="add|change", KERNEL=="xvd[a-z]|[sv]d[a-z]", ATTR{queue/rotational}=="1", ATTR{queue/scheduler}="deadline"

{% if 'storage' in group_names %}
## set disk tweaks
## see: http://www.fhgfs.com/wiki/wikka.php?wakka=StorageServerTuning
## see: http://community.gluster.org/a/linux-kernel-tuning-for-glusterfs/
# give the IO scheduler more flexibility by increasing the number of schedulable requests (queue_depth * 2):
ACTION=="add|change", KERNEL=="sd[a-z]", ATTR{queue/nr_requests}="2048"
# To improve throughput for sequential reads, increase the maximum amount of read-ahead data. The actual amount of read-ahead is adaptive, so using a high value here won't harm performance for small random access.
# According to RedHat Storage best practices and rhs-high-throughput tuned profile:
#
#    this seems huge but for a modern RAID volume that can do 1 GB/s,
#    this is just 65 msec of data, enough to optimize away some seeks on
#    a multi-stream workload but not enough to kill response time
#
ACTION=="add|change", KERNEL=="sd[a-z]", ATTR{queue/read_ahead_kb}="65536"
{% endif %}

{# just in case the host doesn't have this defined, can't go in role defaults because it messes up inheritance for some playbooks #}
{% set nginx_server_name    = nginx_server_name | default("localhost") %}
{% set nginx_vhosts         = nginx_vhosts | default(nginx_server_name) %}

{% if 'library.cgiar.org' in nginx_vhosts -%}
    {% include 'nginx/library.cgiar.org.conf.j2' %}
{% endif %}

{% if 'mahider.ilri.org' in nginx_vhosts and 'dspace.ilri.org' in nginx_vhosts -%}
    {% include 'nginx/legacy-dspace-domains.conf.j2' %}
{% endif %}

# Create a new cache for rest requests with space for 80,000 keys (one megabyte
# can store about eight thousand keys), purges inactive responses after 24 hours
# regardless of their validity, and has two levels of storage, ie "2c/65".
#
# See: https://nginx.org/en/docs/http/ngx_http_proxy_module.html
proxy_cache_path /var/cache/nginx/rest_cache levels=2:2 keys_zone=rest:10m inactive=24h max_size=1G;

{% if nginx_tls_cert is defined %}
# Handle {{ nginx_server_name }} http -> https
#
server {
    listen 80;
    listen [::]:80;
    server_name {{ nginx_server_name }};

    # redirect http -> https
    location / {
        # log access requests for debug / load analysis
        access_log /var/log/nginx/http-access.log;

        # ? in rewrite makes sure nginx doesn't append query string again
        # see: http://wiki.nginx.org/NginxHttpRewriteModule#rewrite
        rewrite ^ https://{{ nginx_server_name }}$request_uri? {% if nginx_redirect_type == 301 %}permanent{% else %}redirect{% endif %};
    }

    include extra-security.conf;
}
{% endif %} {# end: nginx_tls_cert #}

# HTTPS server (only for domains we have a cert for)
#
server {
    {% if nginx_tls_cert is defined %}
    listen {{ nginx_ssl_port }} ssl http2;
    listen [::]:{{ nginx_ssl_port }} ssl http2;
    {% else %}
    listen 80;
    listen [::]:80;
    {% endif %} {# end: nginx_tls_cert #}

    server_name {{ nginx_server_name }};

    {% if nginx_forbid_robots == True %}
    # Add header to forbid robots to index
    # See: https://developers.google.com/webmasters/control-crawl-index/docs/robots_meta_tag
    add_header X-Robots-Tag "none";
    {% endif %}

    ssl_certificate {{ nginx_tls_cert }};
    ssl_certificate_key {{ nginx_tls_key }};

    include /etc/nginx/intermediate-tls.conf;

    # permanent some legacy DSpace 6 URLs for user convenience
    rewrite ^/discover$ https://{{ nginx_server_name }}/search permanent;
    rewrite ^/recent-submissions$ https://{{ nginx_server_name }}/search?spc.sf=dc.date.accessioned&spc.sd=DESC permanent;
    rewrite ^/(ldap|password)-login$ https://{{ nginx_server_name }}/login permanent;

    # Custom JSON response for 429 errors
    error_page 429 = @429;
    location @429 {
        default_type application/json;
        return 429 '{"status": 429, "message": "Too Many Requests"}';
    }

    # rate limit requests to dynamic pages to protect against bots and malicious
    # users (see the definition for the dynamicpages limit_req_zone later).
    location ~ ^/(browse|search) {
        if ($http_user_agent = '') {
            return 403 "Due to abuse we no longer permit requests without a user agent. Please specify a descriptive user agent, for example containing the word 'bot', if you are accessing the site programmatically. For more information see here: https://$server_name/page/about.";
        }

        # log access requests for debug / load analysis
        access_log /var/log/nginx/https-access.log;

        # rate limit for dynamic pages
        limit_req zone=dynamicpages burst=5;

        # rate limit for poorly behaved bots, see limit_req_zone below
        limit_req zone=badbots_ua;
        limit_req zone=badbots_ip;

        # Set the status code for rate-limited requests
        limit_req_status 429;

        # Log at WARN level instead of ERROR
        limit_req_log_level warn;

        # Never index dynamic pages!
        add_header X-Robots-Tag "none";

        # Explicitly set proxy headers again because the existing headers from
        # the global context get cleared when we add new headers with add_header
        # and proxy_set_header in this block.
        # See: https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_set_header
        include proxy_params;

        # Send requests to Express
        proxy_pass http://express-http;
        # Set user agent string to mapped value (see mapping block)
        proxy_set_header User-Agent $ua;
    }

    location / {
        if ($http_user_agent = '') {
            return 403 "Due to abuse we no longer permit requests without a user agent. Please specify a descriptive user agent, for example containing the word 'bot', if you are accessing the site programmatically. For more information see here: https://$server_name/page/about.";
        }

        # log access requests for debug / load analysis
        access_log /var/log/nginx/https-access.log;

        # rate limit for poorly behaved bots, see limit_req_zone below
        limit_req zone=badbots_ua;
        limit_req zone=badbots_ip;

        # Set the status code for rate-limited requests
        limit_req_status 429;

        # Log at WARN level instead of ERROR
        limit_req_log_level warn;

        # Explicitly set proxy headers again because the existing headers from
        # the global context get cleared when we add new headers with add_header
        # and proxy_set_header in this block.
        # See: https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_set_header
        include proxy_params;

        # Send requests to Express
        proxy_pass http://express-http;
        # Set user agent string to mapped value (see mapping block)
        proxy_set_header User-Agent $ua;
    }

    # log API requests
    location /server {
        access_log /var/log/nginx/api-access.log;

        # Explicitly set proxy headers again because the existing headers from
        # the global context get cleared when we add new headers with add_header
        # and proxy_set_header in this block.
        # See: https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_set_header
        include proxy_params;

        # Send requests to Tomcat
        proxy_pass http://tomcat-http;
        # Set user agent string to mapped value (see mapping block)
        proxy_set_header User-Agent $ua;
    }

    # Explicitly allow anyone to request robots.txt and the sitemaps
    location ~ ^/(robots\.txt|sitemap*) {
        # log access requests for debug / load analysis
        access_log /var/log/nginx/https-access.log;

        # Explicitly set proxy headers again because the existing headers from
        # the global context get cleared when we add new headers with add_header
        # and proxy_set_header in this block.
        # See: https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_set_header
        include proxy_params;

        # Send requests to Express
        proxy_pass http://express-http;
        # Set user agent string to mapped value (see mapping block)
        proxy_set_header User-Agent $ua;
    }

    # static alias for weekly IWMI CSV export
    location /iwmi.csv {
        alias {{ dspace_root }}/exports/iwmi.csv;
    }

    # AReS explorer
    location /explorer {
        # log access requests for debug / load analysis
        access_log /var/log/nginx/https-access.log;

        # explicitly set the Host header before proxying to upstream nginx
        proxy_set_header Host {{ dspace_explorer_server }};
        proxy_pass https://codeobia_ares/explorer;
    }

    # log legacy rest requests
    location /rest {
        if ($http_user_agent = '') {
            return 403 "Due to abuse we no longer permit requests without a user agent. Please specify a descriptive user agent, for example containing the word 'bot', if you are accessing the site programmatically. For more information see here: https://$server_name/page/about.";
        }
        # redirect invalid REST retrieveLink values (see DS-3193)
        rewrite ^/rest/rest/(.*)$ https://{{ nginx_server_name }}/rest/$1 redirect;

        location ~ /rest/statistics/?(.*) {
            access_log /var/log/nginx/statistics.log;
            proxy_pass http://statistics_api/$1$is_args$args;
        }

        access_log /var/log/nginx/rest-access.log;

        # Location for rest requests that we want to cache (see proxy_cache_path
        # above). Most importantly we only want to match requests that are from
        # bots or other programmatic clients, not those used by actual users be-
        # cause we need to make sure Solr registers the statistics hit.
        #
        # So for example we want to match these:
        #
        #   /rest/handle/10568/27611
        #   /rest/handle/10568/98424?expand=all
        #   /rest/items/3f692ddd-7856-4bf0-a587-99fb3df0688a/metadata
        #   /rest/items/b014e36f-b496-43d8-9148-cc9db8a6efac/bitstreams
        #   /rest/items?expand=metadata,parentCommunityList,parentCollectionList,bitstreams&limit=10&offset=36270
        #
        # But not these:
        #
        #   /rest/bitstreams/15412/retrieve
        #   /rest/rest/bitstreams/28926633-c7c2-49c2-afa8-6d81cadc2316/retrieve
        #
        # See: https://regex101.com/r/vPz11y/1
        location ~ /rest/(handle|items|collections|communities)/? {
            proxy_cache rest;
            # How long to cache valid HTTP responses (default 200, 301, 302)
            proxy_cache_valid 24h;
            # Allow use of stale entries if the cache is updating
            proxy_cache_use_stale updating;
            # Override the default proxy cache key to include the HTTP Accept
            # header. This makes sure we don't send JSON responses to clients
            # requesting XML and vice versa.
            proxy_cache_key $scheme$proxy_host$request_uri$http_accept;
            # DSpace always sends a cookie and cache control headers, which makes
            # nginx refuse to cache the response.
            proxy_ignore_headers "Set-Cookie" "Cache-Control";
            # Add header with cache status (MISS, HIT, EXPIRED, etc.)
            add_header X-Cache-Status $upstream_cache_status;
            # Make sure session cookies are not leaked in cached payloads
            proxy_hide_header "Set-Cookie";

            # Don't cache when user Shift-refreshes (Cache-Control: no-cache) or
            # when a client has an active session (see the $cookie_jsessionid map).
            proxy_cache_bypass $http_cache_control $active_user_session;
            proxy_no_cache $http_cache_control $active_user_session;

            # Explicitly set proxy headers again because the existing headers from
            # the global context get cleared when we add new headers with add_header
            # and proxy_set_header in this block.
            # See: https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_set_header
            include proxy_params;

            proxy_pass http://tomcat-http;
            # Set user agent string to mapped value (see mapping block)
            proxy_set_header User-Agent $ua;
        }

        # Explicitly set proxy headers again because the existing headers from
        # the global context get cleared when we add new headers with add_header
        # and proxy_set_header in this block.
        # See: https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_set_header
        include proxy_params;

        proxy_pass http://tomcat-http;
        # Set user agent string to mapped value (see mapping block)
        proxy_set_header User-Agent $ua;
    }

    include extra-security.conf;
}

upstream tomcat-http {
    {% if nginx_tls_cert is defined %}
    server 127.0.0.1:8443;
    {% else %}
    server 127.0.0.1:8001;
    {% endif %}

    # The keepalive parameter sets the maximum number of idle keepalive connections
    # to upstream servers that are preserved in the cache of each worker process. When
    # this number is exceeded, the least recently used connections are closed.
    keepalive 60;
}

# dspace-angular running via Node.js with Express
upstream express-http {
    server 127.0.0.1:4000;

    # The keepalive parameter sets the maximum number of idle keepalive connections
    # to upstream servers that are preserved in the cache of each worker process. When
    # this number is exceeded, the least recently used connections are closed.
    keepalive 60;
}

upstream codeobia_ares {
    server {{ dspace_explorer_server }}:443;

    # The keepalive parameter sets the maximum number of idle keepalive connections
    # to upstream servers that are preserved in the cache of each worker process. When
    # this number is exceeded, the least recently used connections are closed.
    keepalive 60;
}

upstream statistics_api {
    server 127.0.0.1:5000;
}

# List of networks to mark as bots. Hosts on these networks are behaving like
# bots but not declaring a bot user agent. We then rely on Tomcat's Crawler
# Session Manager Valve to assign one single session to all these users in
# order to reduce resource usage.
#
# In the included file, matching networks have the key set to 'bot', which we
# map to $limit_bots_ip and use as the key for a limit_req_zone below for bots.
# Networks that don't match have $limit_bots_ip set to an empty string, which
# is importing for the limit_req zone and the next mapping block.
geo $limit_bots_ip {
    # requests with an empty key are not evaluated by limit_req
    # see: http://nginx.org/en/docs/http/ngx_http_limit_req_module.html
    default '';

    # Unblock ourselves
    {{ ansible_default_ipv6.address }} '';

    # Unblock AReS harvester
    157.90.157.100 '';

    include /etc/nginx/bot-networks.conf;
}

# Matching networks have the key set to 'bot' by the geo module above, while
# those not matching are set to an empty string. We re-use this below by map-
# ping $limit_bots_ip to $ua, which we then pass to Tomcat via proxy_set_header
# in location blocks above.
#
# Note: this seems overly complicated, but is necessary because geo does not
# support dynamic values. Instead we set an empty string above and then map
# the host's declared user agent to $ua when we see an empty string. This is
# also a way for me to use one bot-networks.conf file for both the limiting
# of requests as well as classifying bots.
map $limit_bots_ip $ua {
    # If the geo block above set this to an empty string then we can map the
    # host's declared user agent directly. Otherwise, it's a bot as declared
    # by the geo block.
    ''      $http_user_agent;

    default 'bot';
}

# Use a mapping to identify certain search bots with many IP addresses and force
# them to obey a global request rate limit. For example, Baidu actually has over
# 160 IP addresses and often crawls the site with fifty or so concurrently! This
# maps all Baidu requests to the same $limit_bots value, allowing us to force it
# to abide by a total rate limit for all client instances regardless of the IP.
#
# $limit_bots_ua will be used as the key for the limit_req_zone.
#
# Note: nginx will use the first matching pattern, so we should define overrides
# before including the list of bots.
map $http_user_agent $limit_bots_ua {
    # requests with an empty key are not evaluated by limit_req
    # see: http://nginx.org/en/docs/http/ngx_http_limit_req_module.html
    default '';

    # 2020-03-10 Explicitly allow UptimeRobot
    ~UptimeRobot '';

    # 2020-06-08 Explicitly allow Altmetric
    ~Altmetribot '';
    ~Postgenomic '';

    # 2020-08-10 Explicitly allow Googlebot and Twitterbot because they don't
    # crawl haphazardly and usually respect robots.txt
    ~Googlebot '';
    ~Twitterbot '';

    # Need to make sure we match: bot, Bot, BOT, etc...
    ~*bot      1;
    ~[Cc]rawl  1;
    ~[Sp]ider  1;

    # 2021-12-29 dead giveway for a bot that is trying to pretend to be a user
    ~randint 1;

    # 2023-01-18 python requests or urllib should be treated as bots in the
    # frontend (we already drop their hits from Solr).
    ~python 1;
}

# Check if the JSESSIONID cookie is present and contains a 32-character hex
# value, which would mean that a user is actively attempting to re-use their
# Tomcat session. Then we set the $active_user_session variable and use it
# to bypass the nginx proxy cache in REST requests. This is safe because any
# client sending one of these has likely already logged in and is trying to
# get fresh data.
map $cookie_jsessionid $active_user_session {
    # requests with an empty key are not evaluated by limit_req
    # see: http://nginx.org/en/docs/http/ngx_http_limit_req_module.html
    default '';

    '~[A-Z0-9]{32}' 1;
}

# Zone for limiting "bad bot" requests with a hard limit of 1 per minute. Uses
# the variable $limit_bots as a key, which is controlled by the mapping above.
# I am using 1 requests per minute because Baidu currently does about 20 or 30,
# but I don't feel like prioritizing their requests because they don't respect
# the instructions in robots.txt. This is probably overkill for just punishing
# Baidu, but I wanted to explore a solution that could work for other bad user
# agents in the future with little adjustments.
#
# A zone key size of 1 megabyte should be able to store around 16,000 sessions.
limit_req_zone $limit_bots_ua zone=badbots_ua:1m rate=1r/m;
limit_req_zone $limit_bots_ip zone=badbots_ip:1m rate=1r/m;

# Zone for limiting requests to dynamic pages like browse and discover that are
# of no use to bots and cause a high load on the server. For now I think I will
# use a limit of 12, because I imagine a human user could legitimately click a
# link every five seconds, but bots like Facebook are requesting 300/minute.
#
# The user's IP address is used as the key.
#
# See: https://www.nginx.com/blog/rate-limiting-nginx/
limit_req_zone $binary_remote_addr zone=dynamicpages:1m rate=12r/m;

# vim: set ts=4 sw=4:
